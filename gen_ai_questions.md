

1. Explain the transformer architecture.
2. How you fine tune an open source model on a custom dataset.
3. What is self attention. Why this really works?
4. What are embeddings, how we create this.
5. How to reduce the 70B paramter model to 8B parameters.
6. What metrics we use to evaluate the performance of fine tuned models.
7.  Quantization techniques
8. What is Multihead Attention and cross attention.
9. What are the challenges while finetuing a model, what strategies can be employed to overcome.
10. Regularization techinques like - Dropout, label smoothing, weight decay.
11. What loss functions we use in case of NLP models training.
